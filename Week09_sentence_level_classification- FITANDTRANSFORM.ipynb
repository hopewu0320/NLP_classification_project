{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7L3cYlZl15_K"
   },
   "source": [
    "# Week 9: Sentence Level Classification with BERT\n",
    "\n",
    "Your goal this week is to train a classifier that can predict the CEFR level of any given sentence. In this notebook we will guide you through the process of using ðŸ¤—[Hugging Face](https://huggingface.co/) and its transformers library as the training framework, with [Pytorch](https://pytorch.org/) as the deep learning backend, but feel free to use [TensorFlow](https://www.tensorflow.org) if that's what you are more familiar with.\n",
    "\n",
    "For this assignment we will provide a dataset containing sentences with the corresponding CEFR level, and you have to use BERT and train a sentence classifier with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8TbxtroCxM8"
   },
   "source": [
    "## Prepare your environment\n",
    "\n",
    "As always, we highly recommend that you install all packages with a virtual environment manager, like [venv](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) or [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html), to prevent version conflicts of different packages.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSHP0CPoXj7Z"
   },
   "source": [
    "### Install CUDA\n",
    "Deep learning is a computionally extensive process. It takes lots of time if relying only on the CPU, especially when it's trained on a large dataset. That's why using GPU instead is generally recommended.  \n",
    "To use GPU for computation, you have to install [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit) as well as the [cuDNN library](https://developer.nvidia.com/cudnn) provided by NVIDIA.  \n",
    "\n",
    "If you already had CUDA installed on your machine, then great! You're done here.  \n",
    "If you don't, you can refer to [Appendix](#Appendix-1-Install-CUDA) to see how to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f78jLXeZfPyH"
   },
   "source": [
    "\n",
    "### Install python packages\n",
    "The following python packages will be used in this tutorial:\n",
    "\n",
    "1. `numpy`: for matrix operation\n",
    "2. `scikit-learn`: for label encoding\n",
    "3. `datasets`: for data preparation\n",
    "4. `transformers`: for model loading and finetuing\n",
    "5. `pytorch`: the backend DL framework\n",
    "  - Note that the pt version must support the CUDA version you've installed if you want to use GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cS9CxjyfQ-F"
   },
   "source": [
    "### Select GPU(s) for your backend\n",
    "\n",
    "Skip this section if you have no intension of using GPU with tensorflow/pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sEJ8Y8SCfWp_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "# select your GPU. Note that this should be set before you load tensorflow or pytorch.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "# To use multiple GPUs, combine all GPU ID with commas\n",
    "# e.g. >>> os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1p_qQKbfcCH",
    "outputId": "d25bdd87-959c-435d-8c57-22f5c350098f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Check if any GPU is used\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P0foxjBDQSu"
   },
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "Before starting the training, we need to load and process our dataset - but wait, let's decide which model we want to use first.  \n",
    "\n",
    "In the highly unlikely chance you've never heard of it, [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) is a language model proposed by Google AI in 2018, and it's currently one of the most popular models used in NLP.  \n",
    "You can learn more about it here:\n",
    "- [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/) by Samia, 2019.\n",
    "\n",
    "\n",
    "However, we will not directly use BERT in this tutorial, because it's large and takes too long to train. Instead, we'll be using [DistilBert](https://medium.com/huggingface/distilbert-8cf3380435b5), a version of BERT that while light-weight, reserves 95% of its original accuracy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Lqb2cHCmDIEp"
   },
   "outputs": [],
   "source": [
    "# the model you want to use. Available models can be found here: https://huggingface.co/models\n",
    "MODEL_NAME = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxSUKsTkDSxJ"
   },
   "source": [
    "### Load data\n",
    "\n",
    "Similar to the `transformers` library, `datasets` is also a package by huggingface. It contains many public datasets online and can help us with the data processing.  \n",
    "We can use `load_dataset` function to read the input `.csv` file provided for this assignment.\n",
    "\n",
    "Reference:\n",
    " - [Official datasets document](https://huggingface.co/docs/datasets)\n",
    " - [datasets.load_dataset](https://huggingface.co/docs/datasets/loading.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hjY9HMNIt5jf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a3ae748087bf9cd4\n",
      "Found cached dataset csv (C:/Users/user/.cache/huggingface/datasets/csv/default-a3ae748087bf9cd4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28c4d457bac4ab79a33d2f904bde91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [ TODO ] load the data using the load_dataset function\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\",data_files=\"data/csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npepcv7GfMHI",
    "outputId": "02ed2940-370f-444c-b748-abcd623891bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'level'],\n",
      "    num_rows: 23020\n",
      "})\n",
      "{'text': \"Unfortunately he was too fast and I couldn't keep up with him.\", 'level': 'B2'}\n",
      "['No longer a remote, backward, unimportant country, it became a force to be reckoned with in Europe.', \"Unfortunately he was too fast and I couldn't keep up with him.\", 'Most mushrooms are totally harmless, but some are poisonous.', 'This provided solid evidence that he committed the crime.', \"You can't just accept everything you read in the newspapers at face value.\"]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'])\n",
    "print(dataset['train'][1])\n",
    "print(dataset['train']['text'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C2', 'B2', 'B2', 'C2', 'C1']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['level'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3olKw19uuJQ"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "As always, texts should be tokenized, embedded, and padded before being put into the model.  \n",
    "But not to worry, there are libraries from huggingface to help with this, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3ANJAV7wn2R"
   },
   "source": [
    "#### Sentence processing\n",
    "\n",
    "Different pre-trained language models may have their own preprocessing models, and that's why we should use the tokenizers trained along with that model. In our case, we are using distilBERT, so we should use the distilBERT tokenizer.  \n",
    "\n",
    "With huggingface, loading different tokenizers is extremely easy: just import the AutoTokenizer from `transformers` and tell it what model you plan to use, and it will handle everything for you.\n",
    "\n",
    "Reference:\n",
    " - [transformers.AutoTokenizer](https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DALrmSh6wpmy"
   },
   "outputs": [],
   "source": [
    "# [ TODO ] load the distilBERT tokenizer using AutoTokenizer\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jd43REmNxCOV"
   },
   "source": [
    "#### Label processing\n",
    "\n",
    "Our labels also need to be processed, so let's do that next.\n",
    "\n",
    "For this tutorial, we'll use the OneHotEncoder provided by scikit-learn.\n",
    "\n",
    "For now, just declare a new encoder and use `fit` to learn the data. Hint: you should still end up with 6 labels.\n",
    "\n",
    "Documents:\n",
    " - [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "q3ml0_WxxFh5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [ TODO ] declare a new encoder and let it learn from the dataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(np.reshape(dataset['train']['level'],(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRY2GDJa1MxF",
    "outputId": "df210570-a2f8-478d-8902-e6f6c4f33af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# check if you still have 6 labels\n",
    "LABEL_COUNT = len(encoder.categories_[0])\n",
    "print(LABEL_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C2'],\n",
       "       ['B2'],\n",
       "       ['B2'],\n",
       "       ...,\n",
       "       ['B1'],\n",
       "       ['B2'],\n",
       "       ['A1']], dtype='<U2')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(dataset['train']['level'],(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrfO8c4R1eWO"
   },
   "source": [
    "#### Process the data\n",
    "\n",
    "To make things easier, we can write a function to process our dataset in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cvwnYLah1dbN"
   },
   "outputs": [],
   "source": [
    "def preprocess(dataslice):\n",
    "    \"\"\" Input: a batch of your dataset\n",
    "        Example: { 'text': [['sentence1'], ['setence2'], ...],\n",
    "                   'label': ['label1', 'label2', ...] }\n",
    "    \"\"\"\n",
    "    oneHot=encoder.transform(np.reshape(dataslice['level'],(-1,1))).toarray()\n",
    "    #print(oneHot)\n",
    "    #label_num=np.argmax(oneHot, axis=1)\n",
    "    \n",
    "    dataslice[\"label\"]=oneHot \n",
    "    return tokenizer(dataslice[\"text\"], padding='max_length', truncation=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\" Output: a batch of processed dataset\n",
    "        Example: { 'input_ids': ...,\n",
    "                   'attention_masks': ...,\n",
    "                   'label': ... }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform(np.reshape(dataset['train']['level'],(-1,1))).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder2 = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder2.fit_transform(np.reshape(dataset['train']['level'],(-1,1))).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2053, 2936, 1037, 6556, 1010, 8848, 1010, 4895, 5714, 6442, 4630, 2406, 1010, 2009, 2150, 1037, 2486, 2000, 2022, 29072, 2098, 2007, 1999, 2885, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_str = tokenizer(dataset['train']['text'][0], padding=True, truncation=True) \n",
    "encoded_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A1', 'A2', 'B1', 'B2', 'C1', 'C2'], dtype='<U2')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.categories_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Nr0-Y1bR2efQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:/Users/user/.cache/huggingface/datasets/csv/default-a3ae748087bf9cd4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-3e881b76b7d4c251.arrow\n"
     ]
    }
   ],
   "source": [
    "# map the function to the whole dataset\n",
    "processed_data = dataset.map(preprocess,    # your processing function\n",
    "                             batched = True, # Process in batches so it can be faster                            \n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[1,2,3],\n",
    "   [4,5,6]\n",
    "  ]\n",
    "np.argmax(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_G5ftJbjfa4i",
    "outputId": "c88d0fc1-85ca-4cdb-e191-a426e29fcfc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'level', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 23020\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'She seemed ideally suited for the job.',\n",
       " 'level': 'B2',\n",
       " 'label': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
       " 'input_ids': [101,\n",
       "  2016,\n",
       "  2790,\n",
       "  28946,\n",
       "  10897,\n",
       "  2005,\n",
       "  1996,\n",
       "  3105,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_data)\n",
    "processed_data['train'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_data.set_format(type='torch', columns=['input_ids', 'level', 'attention_mask', 'label'])\n",
    "#tokenized_datasets = tokenized_datasets.remove_columns_(dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9z7ZMtP22b9"
   },
   "source": [
    "### DataCollator\n",
    "\n",
    "You might have noticed that we skipped padding the sentences. That's because we are going to do it during training.  \n",
    "\n",
    "To do training-time processing, we can use the DataCollator Class provided by `transformers`. And guess what - transformers has a class that will handle padding for us, too!\n",
    "\n",
    " - [transformers.DataCollatorWithPadding](https://huggingface.co/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "x5orGYjN39dz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=PreTrainedTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [ TODO ] declare a collator to do padding during traning\n",
    "# https://zhuanlan.zhihu.com/p/414552021\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di75QVgv4V81"
   },
   "source": [
    "## Training\n",
    "\n",
    "Finally, we can move on to training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53dO3pg85u0n"
   },
   "source": [
    "### Preparation\n",
    "\n",
    "We can load the pretrained model from `transformers`.  \n",
    "Generally, you need to build your own model on top of BERT if you want to use BERT for some downstream tasks, but again, sequence classification is a popular topic. With the support from `transformers` library, it can be done in two lines of codes: \n",
    "\n",
    "1. Load `AutoModelForSequenceClassification` Class.\n",
    "2. Load the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'label2id, id2label = dict(), dict()\\nfor i, label in enumerate(encoder.categories_[0]):\\n    label2id[label] = str(i)\\n    id2label[str(i)] = label'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(encoder.categories_[0]):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UyDyv7wp5qdD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
    "                                                           num_labels = LABEL_COUNT,\n",
    "                                                           #label2id=label2id,\n",
    "                                                           #id2label=id2label\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtmP4TEh6XiB"
   },
   "source": [
    "#### Split train/val data\n",
    "\n",
    "The `Dataset` class we prepared before has a `train_test_split` method. You can use it to split your (processed) dataset.\n",
    "\n",
    "Document:\n",
    " - [datasets.Dataset - Sort, shuffle, select, split, and shard](https://huggingface.co/docs/datasets/process.html#sort-shuffle-select-split-and-shard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NnbD1KW16YWn"
   },
   "outputs": [],
   "source": [
    "# [ TODO ] choose a validation size and split your data\n",
    "train_val_dataset = processed_data['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6u93fCpofgbe",
    "outputId": "65b7044c-6a01-4c07-f456-75d0e83d9d70",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'level', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 18416\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'level', 'label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 4604\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzmSbOaD7O0F"
   },
   "source": [
    "#### Setup training parameters\n",
    "\n",
    "We are using the TrainerAPI to do the training. Trainer is yet another utility provided by huggingface, which helps you train the model with ease.  \n",
    "\n",
    "Document:\n",
    "- [transformers.TrainingArguments](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "- [transformers.Trainer](https://huggingface.co/docs/transformers/master/en/main_classes/trainer#transformers.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ABqlinlO76Ax"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5lzXTG1y7q7n"
   },
   "outputs": [],
   "source": [
    "# [ TODO ] set and tune your training properties\n",
    "OUTPUT_DIR = \"data/out.txt\"\n",
    "LEARNING_RATE = 0.1\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 40\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = OUTPUT_DIR,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    per_device_eval_batch_size = BATCH_SIZE,\n",
    "    num_train_epochs = EPOCH,\n",
    "    # you can set more parameters here if you want\n",
    ")\n",
    "\n",
    "# now give all the information to a trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_val_dataset['train'],\n",
    "    eval_dataset=train_val_dataset['test']\n",
    "    # set your parameters here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE0DpS3s7rhg"
   },
   "source": [
    "### Training\n",
    "\n",
    "This is the easy part. Simply ask the trainer to train the model for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "wsrQOyJCiFas"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep\\tTraining Loss\\n500\\t0.397600\\n1000\\t0.357100\\n1500\\t0.349800\\n2000\\t0.339100\\n2500\\t0.310500\\n3000\\t0.257100\\n3500\\t0.256400\\n4000\\t0.253900\\n4500\\t0.241700\\n5000\\t0.184100\\n5500\\t0.156000\\n6000\\t0.146900\\n6500\\t0.144500\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://blog.csdn.net/weixin_41868756/article/details/122961147\n",
    "#hidden\n",
    "'''\n",
    "Step\tTraining Loss\n",
    "500\t0.397600\n",
    "1000\t0.357100\n",
    "1500\t0.349800\n",
    "2000\t0.339100\n",
    "2500\t0.310500\n",
    "3000\t0.257100\n",
    "3500\t0.256400\n",
    "4000\t0.253900\n",
    "4500\t0.241700\n",
    "5000\t0.184100\n",
    "5500\t0.156000\n",
    "6000\t0.146900\n",
    "6500\t0.144500\n",
    "'''\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JBUA0pe-S9b"
   },
   "source": [
    "### Save for future use\n",
    "\n",
    "Hint: try using `save_pretrained`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e30uXCf-cXc",
    "outputId": "f003dfa4-060a-4419-9ba6-f559029e5402"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model_path='./data/model'\\nmodel.save_pretrained(model_path)\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [ TODO ] practice saving your model for future use\n",
    "#hidden\n",
    "'''model_path='./data/model'\n",
    "model.save_pretrained(model_path)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QSZjlcG9fOk"
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Now we know exactly how to train a model, but how do we use it for predicting results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7akP5Hh9ugG"
   },
   "source": [
    "### Load finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Lfb_zJGm9vJP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./data/model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./data/model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./data/model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./data/model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# [ TODO ] load the model that you saved\n",
    "# https://stackoverflow.com/questions/72108945/saving-finetuned-model-locally\n",
    "\n",
    "\n",
    "model_path='./data/model'\n",
    "mymodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6Vs3iBE_nck"
   },
   "source": [
    "### Get the prediction\n",
    "\n",
    "Here are a few example sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "FtLt6IBi_pLF"
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    # A2\n",
    "    \"Remember to write me a letter.\",\n",
    "    # B2\n",
    "    \"Strawberries and cream - a perfect combination.\",\n",
    "    \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\",\n",
    "    # C1\n",
    "    \"Some may altogether give up their studies, which I think is a disastrous move.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xhk7ZRX_2U2"
   },
   "source": [
    "All we need to do is to transform them to embeddings, and then we can get predictions by calling your finetuned model.  \n",
    "\n",
    "Since we don't have a DataCollator to pad the sentence and do the matrix transformation this time, we have to pad and transform the matrice on our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWONG_lCAkyN",
    "outputId": "0c408ae7-997d-4c0c-8f49-c6196397602f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5930,  0.7402, -4.4594, -5.6575, -5.8491, -6.1265],\n",
       "        [-6.9154, -6.3664, -3.0927,  1.6226, -3.7276, -2.8576],\n",
       "        [-7.5301, -7.0676, -3.6806,  3.0856, -4.1731, -4.8100],\n",
       "        [-6.9661, -6.6125, -5.2798, -3.6026,  2.8461, -3.5893]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the sentences into embeddings\n",
    "input = tokenizer(examples, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "# Get the output\n",
    "logits = mymodel(**input).logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBMUBD-1BFW1"
   },
   "source": [
    "Logits aren't very readable for us. Let's use softmax \n",
    "activation to transform them into more probability-like numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjiKxLaBBGah",
    "outputId": "006d24be-ed4f-48cb-ab2c-fa7b70b44dd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0707e-01, 7.8539e-01, 4.3346e-03, 1.3080e-03, 1.0799e-03, 8.1833e-04],\n",
       "        [1.9100e-04, 3.3071e-04, 8.7337e-03, 9.7507e-01, 4.6289e-03, 1.1049e-02],\n",
       "        [2.4473e-05, 3.8862e-05, 1.1494e-03, 9.9771e-01, 7.0243e-04, 3.7153e-04],\n",
       "        [5.4584e-05, 7.7732e-05, 2.9473e-04, 1.5769e-03, 9.9640e-01, 1.5979e-03]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "predicts = nn.functional.softmax(logits, dim = -1) \n",
    "predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAqgoJTFBchb"
   },
   "source": [
    "#### Transform logits back to labels\n",
    "\n",
    "Now you've got the output. Write a function to map it back into labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvcyotBfBifR",
    "outputId": "50cae16f-06ed-4f04-c684-496044f37233"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A2', 'B2', 'B2', 'C1'], dtype='<U2')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [ TODO ] try to process the result\n",
    "def Decoder(encoder,predicts):\n",
    "    label=encoder.inverse_transform(predicts.detach().numpy())\n",
    "    label=np.reshape(label,(1,-1))\n",
    "    return label[0]\n",
    "\n",
    "Decoder(encoder,predicts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfCuP95IBvP-"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's see how you did!  \n",
    "Load the testing data and calculate your accuracy.\n",
    "\n",
    "We want you to calculate the three kinds of accuracy mentioned in the lecture, which will also be explained in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You must have a firm, outgoing personality, but be self-reliant and strong-willed.',\n",
       " 'I like living on my own.',\n",
       " 'I think this milk is bad.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_dataset['test']['text'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'A1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'A2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'C1',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'B1',\n",
       " 'C1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A2',\n",
       " 'B1',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B1',\n",
       " 'B1',\n",
       " 'B2',\n",
       " 'C2',\n",
       " 'C2',\n",
       " 'B2',\n",
       " 'A1',\n",
       " 'C2',\n",
       " 'B1',\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test ground truth\n",
    "test_ground_truth=train_val_dataset['test']['level']\n",
    "test_ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "C8hUc4sLCW40"
   },
   "outputs": [],
   "source": [
    "# [ TODO ] \n",
    "# load test data\n",
    "# preprocess\n",
    "# get predictions\n",
    "# transform predictions back into labels\n",
    "#test predict\n",
    "with torch.no_grad():\n",
    "    test_input=tokenizer(train_val_dataset['test']['text'], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    test_logits = mymodel(**test_input).logits\n",
    "    predict_ = nn.functional.softmax(test_logits, dim = -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C2', 'B1', 'A2', ..., 'C1', 'C2', 'C1'], dtype='<U2')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label = Decoder(encoder,predict_)\n",
    "predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoC2EAHxCXdj",
    "outputId": "cc711f3c-c789-4d0f-d92f-adf04dc54432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2: C2\n",
      "B1: B1\n",
      "A2: B2\n",
      "C2: C2\n",
      "C1: C1\n",
      "C1: C1\n",
      "B1: B1\n",
      "B2: C1\n",
      "B2: C1\n",
      "B2: B2\n"
     ]
    }
   ],
   "source": [
    "#  try printing out some predictions to check if the outputs are reasonable and if you need to adjust your model at the end of every step.\n",
    "\n",
    "for idx, (sent, level) in enumerate(zip(test_ground_truth, predict_label)):\n",
    "    if idx >= 10: break\n",
    "    print(f'{level}: {sent}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBnEzAFhC7ZN"
   },
   "source": [
    "### Six Level Accuracy\n",
    "\n",
    "Exact accuracy is probably what you're most familiar with:\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#exactly\\:the\\:same\\:levels}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   A1 A2 B1 B2 C1 C2\n",
    "Ground truth: A2 B1 B1 B2 B2 C2\n",
    "                    ^  ^     ^\n",
    "```\n",
    "\n",
    "The six level accuracy is $\\frac{3}{6} = 0.5$\n",
    "\n",
    "As the requirement, <u>your exact accuracy should be higher than $0.5$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR2oVECyC8vD",
    "outputId": "72cebbe0-5ada-446b-b5f7-6a9849292792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.8307993049522154\n"
     ]
    }
   ],
   "source": [
    "# [ TODO ] calculate accuracy\n",
    "'''\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 40\n",
    "Accuracy=0.5612510860121633\n",
    "'''\n",
    "total=len(predict_label)\n",
    "correct=0\n",
    "for idx, (sent, level) in enumerate(zip(test_ground_truth, predict_label)):\n",
    "    if sent==level:\n",
    "        correct+=1\n",
    "accuracy=correct/total\n",
    "print('Accuracy:{}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "We865ayVC_N7"
   },
   "source": [
    "### Three Level Accuracy\n",
    "\n",
    "Three Level Accuracy is used when you only want a more general sense of right or wrong.\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#the\\:same\\:ABC\\:levels}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   A1 A2 B1 B2 C1 C2\n",
    "Ground truth: A2 B1 B1 B2 B2 C2\n",
    "              ^     ^  ^     ^\n",
    "```\n",
    "\n",
    "The three level accuracy is $\\frac{4}{6} = 0.667$\n",
    "\n",
    "As the requirement, <u>your exact accuracy should be higher than $0.6$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCAKM9MRDCBk",
    "outputId": "f599130e-de22-4b28-a521-f378afd52711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9057341442224153\n"
     ]
    }
   ],
   "source": [
    "# [ TODO ] calculate accuracy\n",
    "'''\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 40\n",
    "Accuracy:0.7454387489139879\n",
    "'''\n",
    "correct=0\n",
    "for idx, (sent, level) in enumerate(zip(test_ground_truth, predict_label)):\n",
    "    if sent=='A1' or sent=='A2':\n",
    "        if level=='A1' or level=='A2':\n",
    "            correct+=1\n",
    "    elif sent=='B1' or sent=='B2':\n",
    "        if level=='B1' or level=='B2':\n",
    "            correct+=1\n",
    "    else:\n",
    "        if level=='C1' or level=='C2':\n",
    "            correct+=1\n",
    "accuracy=correct/total\n",
    "print('Accuracy:{}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YSo46NX7nvb"
   },
   "source": [
    "### Fuzzy accuracy\n",
    "\n",
    "However, the level of a sentence is relatively subjective. Generally speaking, $\\pm1$ errors are allowed in the real evaluation in linguistic area.  \n",
    "\n",
    "For example, if the actual label is 'B1', we'll also consider the prediction 'right' if the model predicts 'B2' or 'A2'.\n",
    "\n",
    "Hence, the fuzzy accuracy is\n",
    "\n",
    "$\n",
    "accuracy = \\frac{\\#good\\:enough\\:answers}{\\#total}\n",
    "$\n",
    "\n",
    "Example:\n",
    "```\n",
    "Prediction:   0 1 2 3 4 5\n",
    "Ground truth: 0 1 1 3 3 3\n",
    "              ^ ^ ^ ^ ^\n",
    "```\n",
    "\n",
    "The fuzzy accuracy is $\\frac{5}{6} = 0.833$\n",
    "\n",
    "As the requirement, <u>your accuracy should be higher than $0.8$</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27fP0BTc73Al",
    "outputId": "e792e699-4e04-4a82-ebff-d133aead0b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.960251954821894\n"
     ]
    }
   ],
   "source": [
    "# [ TODO ] calculate accuracy\n",
    "'''\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 40\n",
    "Accuracy:0.8746741963509991\n",
    "'''\n",
    "correct=0\n",
    "for idx, (sent, level) in enumerate(zip(test_ground_truth, predict_label)):\n",
    "    if sent=='A1':\n",
    "        if level=='A1' or level=='A2':\n",
    "            correct+=1\n",
    "    elif sent=='A2' :\n",
    "        if level=='A1' or level=='A2' or level=='B1':\n",
    "            correct+=1\n",
    "    elif sent=='B1' :\n",
    "        if level=='A2' or level=='B1' or level=='B2':\n",
    "            correct+=1\n",
    "    elif sent=='B2' :\n",
    "        if level=='B1' or level=='B2' or level=='C1':\n",
    "            correct+=1\n",
    "    elif sent=='C1' :\n",
    "        if level=='B2' or level=='C1' or level=='C2':\n",
    "            correct+=1\n",
    "    elif sent=='C2' :\n",
    "        if level=='C1' or level=='C2' :\n",
    "            correct+=1\n",
    "accuracy=correct/total\n",
    "print('Accuracy:{}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPrSLQEDDfeE"
   },
   "source": [
    "## TA's Note\n",
    "\n",
    "Congratulations, you made it to the end of the tutorial! Make sure you make an appointment to show your work and turn in your finished assignment before next week's lesson. We will ask you to run your code, so double check that everything is working and that your model is saved. Don't worry if you didn't pass the evaluation requirements, you'll still get partial points for trying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL7CAtRQbR7s"
   },
   "source": [
    "## Appendix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqqUYYPEanAF"
   },
   "source": [
    "\n",
    "<a name=\"Appendix-1-Install-CUDA\"></a>\n",
    "\n",
    "### Appendix 1 - Install CUDA\n",
    "\n",
    "1. Check your GPU vs. CUDA compatibility:\n",
    "   - [NVIDIA -> Your GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) -> GeForce and TITAN Products\n",
    "2. Check library vs. CUDA compatibility: \n",
    "   - Pytorch: [Previous PyTorch Versions](https://pytorch.org/get-started/previous-versions/)\n",
    "   - Tensorflow: [Linux/MacOX](https://www.tensorflow.org/install/source#tested_build_configurations) or [Windows](https://www.tensorflow.org/install/source_windows#tested_build_configurations)\n",
    "3. Note the highest CUDA version that fits your system.\n",
    "\n",
    "#### >> for conda/mamba users\n",
    "\n",
    "You can directly install CUDA library with the selected CUDA version.\n",
    "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
    "2. `conda/mamba install -c conda-forge cudatoolkit=${VERSION}`\n",
    "\n",
    "#### >> for non-conda users\n",
    "\n",
    "1. Get [the driver for NVIDIA GPU](https://www.nvidia.com/download/index.aspx)\n",
    "2. Download and install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
    "3. Download and install [cuDNN Library](https://developer.nvidia.com/rdp/cudnn-archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hFAM1Cya4_c"
   },
   "source": [
    "### Appendix 2 - Further Readings\n",
    "\n",
    "1. [Huggingface Official Tutorials](https://github.com/huggingface/notebooks/tree/master/examples)\n",
    "2. How to use Bert with other downstream tasks: [How to use BERT from the Hugging Face transformer library](https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209): \n",
    "3. Training with pytorch backend: [transformers-tutorials](https://github.com/abhimishra91/transformers-tutorials)\n",
    "4. A more complicated example that include manual data/training processing with Pytorch: [Transformers for Multi-Label Classification made simple](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1)\n",
    "5. [Text Classification with tensorflow](https://github.com/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb): tensorflow example"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
